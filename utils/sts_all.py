import os
import utils
import collections

import numpy as np


class STSAll(object):
    def __init__(self, train_validation_split=None, test_split=None,
                 use_defaults=True):
        if train_validation_split is not None or test_split is not None or \
                use_defaults is False:
            raise NotImplementedError('This Dataset does not implement '
                  'train_validation_split, test_split or use_defaults as the '
                  'dataset is big enough and uses dedicated splits from '
                  'the original datasets')
        self.dataset_name = 'Semantic Text Similarity - All'
        self.dataset_description = 'This dataset has been generated by ' \
               'merging MPD, SICK, Quora, StackExchange and and SemEval ' \
               'datasets. \n It has 258537 Training sentence pairs, 133102 ' \
               'Test sentence pairs and 59058 validation sentence pairs.'
        self.test_split = 'large'
        self.dataset_path = os.path.join(utils.data_root_directory, 'mpd')
        self.train_path = os.path.join(self.dataset_path, 'train', 'train.txt')
        self.validation_path = os.path.join(self.dataset_path, 'validation',
                                            'validation.txt')
        self.test_path = os.path.join(self.dataset_path, 'test', 'test.txt')
        self.vocab_path = os.path.join(self.dataset_path, 'vocab.txt')
        self.w2v_path = os.path.join(self.dataset_path, 'w2v.npy')

        self.w2i, self.i2w = utils.load_vocabulary(self.vocab_path)
        self.w2v = self.load_w2v()

        self.vocab_size = len(self.w2i)
        self.train = DataSet(self.train_path, (self.w2i, self.i2w))
        self.validation = DataSet(self.validation_path, (self.w2i, self.i2w))
        self.test = DataSet(self.test_path, (self.w2i, self.i2w))
        self.__refresh(load_w2v=False)

    def create_vocabulary(self, min_frequency=5, tokenizer='spacy',
                          downcase=True, max_vocab_size=None,
                          name='new', load_w2v=True):
        self.vocab_path, self.w2v_path = utils.new_vocabulary(
                min_frequency, tokenizer=tokenizer, downcase=downcase,
                max_vocab_size=max_vocab_size, name=name)
        self.__refresh(load_w2v)

    def __refresh(self, load_w2v):
        self.w2i, self.i2w = utils.load_vocabulary(self.vocab_path)
        if load_w2v:
            self.w2v = utils.preload_w2v(self.w2i)
        self.train.set_vocab((self.w2i, self.i2w))
        self.validation.set_vocab((self.w2i, self.i2w))
        self.test.set_vocab((self.w2i, self.i2w))

    def load_w2v(self):
        return np.load(self.w2v_path)

    def save_w2v(self, w2v):
        return np.save(self.w2v_path, w2v)


class DataSet(object):
    def __init__(self, path, vocab):

        self.path = path
        self._epochs_completed = 0
        self._index_in_epoch = 0
        self.vocab_w2i = vocab[0]
        self.vocab_i2w = vocab[1]
        self.datafile = None
        self.Batch = collections.namedtuple('Batch', ['s1', 's2', 'sim'])

    def open(self):
        self.datafile = open(self.path, 'r')

    def close(self):
        self.datafile.close()

    def next_batch(self, batch_size=64, balance=True, seq_begin=False,
                   seq_end=False):
        if not self.datafile:
            raise Exception('The dataset needs to be open before being used. '
                            'Please call dataset.open() before calling '
                            'dataset.next_batch()')

        s1s, s2s, sims = [], [], []

        while len(s1s) == batch_size:
            row = self.datafile.readline()
            if row == '':
                self._epochs_completed += 1
                self.datafile.seek(0)
                continue
            cols = row.strip().split('\t')
            s1, s2, sim = cols[0], cols[1], float(cols[2])
            s1, s2 = s1.split(' '), s2.split(' ')
            s1s.append(s1)
            s2s.append(s2)
            sims.append(sim)
        batch = self.Batch(
            s1=utils.padseq(utils.seq2id(s1s[:batch_size],
                                         self.vocab_w2i)),
            s2=utils.padseq(utils.seq2id(s2s[:batch_size],
                                         self.vocab_i2w)),
            sim=sims[:batch_size])
        return batch

    def set_vocab(self, vocab):
        self.vocab_w2i = vocab[0]
        self.vocab_i2w = vocab[1]

    @property
    def epochs_completed(self):
        return self._epochs_completed

